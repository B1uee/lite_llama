my LLama parameters

```bash
tok_embeddings.weight torch.Size([128256, 2048])
norm_weight torch.Size([2048])
lm_head_weight torch.Size([2048, 128256])
layers.0.post_attention_weight torch.Size([2048])
layers.0.input_layernorm_weight torch.Size([2048])

layers.0.attention.q_proj_weight torch.Size([2048, 2048])
layers.0.attention.k_proj_weight torch.Size([2048, 2048])
layers.0.attention.v_proj_weight torch.Size([2048, 2048])
layers.0.attention.o_proj_weight torch.Size([2048, 2048])
layers.0.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.0.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.0.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.1.post_attention_weight torch.Size([2048])
layers.1.input_layernorm_weight torch.Size([2048])
layers.1.attention.q_proj_weight torch.Size([2048, 2048])
layers.1.attention.k_proj_weight torch.Size([2048, 2048])
layers.1.attention.v_proj_weight torch.Size([2048, 2048])
layers.1.attention.o_proj_weight torch.Size([2048, 2048])

layers.1.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.1.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.1.feed_forward.down_proj_weight torch.Size([2048, 8192])

layers.2.post_attention_weight torch.Size([2048])
layers.2.input_layernorm_weight torch.Size([2048])
layers.2.attention.q_proj_weight torch.Size([2048, 2048])
layers.2.attention.k_proj_weight torch.Size([2048, 2048])
layers.2.attention.v_proj_weight torch.Size([2048, 2048])
layers.2.attention.o_proj_weight torch.Size([2048, 2048])
layers.2.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.2.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.2.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.3.post_attention_weight torch.Size([2048])
layers.3.input_layernorm_weight torch.Size([2048])
layers.3.attention.q_proj_weight torch.Size([2048, 2048])
layers.3.attention.k_proj_weight torch.Size([2048, 2048])
layers.3.attention.v_proj_weight torch.Size([2048, 2048])
layers.3.attention.o_proj_weight torch.Size([2048, 2048])
layers.3.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.3.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.3.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.4.post_attention_weight torch.Size([2048])
layers.4.input_layernorm_weight torch.Size([2048])
layers.4.attention.q_proj_weight torch.Size([2048, 2048])
layers.4.attention.k_proj_weight torch.Size([2048, 2048])
layers.4.attention.v_proj_weight torch.Size([2048, 2048])
layers.4.attention.o_proj_weight torch.Size([2048, 2048])
layers.4.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.4.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.4.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.5.post_attention_weight torch.Size([2048])
layers.5.input_layernorm_weight torch.Size([2048])
layers.5.attention.q_proj_weight torch.Size([2048, 2048])
layers.5.attention.k_proj_weight torch.Size([2048, 2048])
layers.5.attention.v_proj_weight torch.Size([2048, 2048])
layers.5.attention.o_proj_weight torch.Size([2048, 2048])
layers.5.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.5.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.5.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.6.post_attention_weight torch.Size([2048])
layers.6.input_layernorm_weight torch.Size([2048])
layers.6.attention.q_proj_weight torch.Size([2048, 2048])
layers.6.attention.k_proj_weight torch.Size([2048, 2048])
layers.6.attention.v_proj_weight torch.Size([2048, 2048])
layers.6.attention.o_proj_weight torch.Size([2048, 2048])
layers.6.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.6.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.6.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.7.post_attention_weight torch.Size([2048])
layers.7.input_layernorm_weight torch.Size([2048])
layers.7.attention.q_proj_weight torch.Size([2048, 2048])
layers.7.attention.k_proj_weight torch.Size([2048, 2048])
layers.7.attention.v_proj_weight torch.Size([2048, 2048])
layers.7.attention.o_proj_weight torch.Size([2048, 2048])
layers.7.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.7.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.7.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.8.post_attention_weight torch.Size([2048])
layers.8.input_layernorm_weight torch.Size([2048])
layers.8.attention.q_proj_weight torch.Size([2048, 2048])
layers.8.attention.k_proj_weight torch.Size([2048, 2048])
layers.8.attention.v_proj_weight torch.Size([2048, 2048])
layers.8.attention.o_proj_weight torch.Size([2048, 2048])
layers.8.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.8.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.8.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.9.post_attention_weight torch.Size([2048])
layers.9.input_layernorm_weight torch.Size([2048])
layers.9.attention.q_proj_weight torch.Size([2048, 2048])
layers.9.attention.k_proj_weight torch.Size([2048, 2048])
layers.9.attention.v_proj_weight torch.Size([2048, 2048])
layers.9.attention.o_proj_weight torch.Size([2048, 2048])
layers.9.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.9.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.9.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.10.post_attention_weight torch.Size([2048])
layers.10.input_layernorm_weight torch.Size([2048])
layers.10.attention.q_proj_weight torch.Size([2048, 2048])
layers.10.attention.k_proj_weight torch.Size([2048, 2048])
layers.10.attention.v_proj_weight torch.Size([2048, 2048])
layers.10.attention.o_proj_weight torch.Size([2048, 2048])
layers.10.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.10.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.10.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.11.post_attention_weight torch.Size([2048])
layers.11.input_layernorm_weight torch.Size([2048])
layers.11.attention.q_proj_weight torch.Size([2048, 2048])
layers.11.attention.k_proj_weight torch.Size([2048, 2048])
layers.11.attention.v_proj_weight torch.Size([2048, 2048])
layers.11.attention.o_proj_weight torch.Size([2048, 2048])
layers.11.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.11.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.11.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.12.post_attention_weight torch.Size([2048])
layers.12.input_layernorm_weight torch.Size([2048])
layers.12.attention.q_proj_weight torch.Size([2048, 2048])
layers.12.attention.k_proj_weight torch.Size([2048, 2048])
layers.12.attention.v_proj_weight torch.Size([2048, 2048])
layers.12.attention.o_proj_weight torch.Size([2048, 2048])
layers.12.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.12.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.12.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.13.post_attention_weight torch.Size([2048])
layers.13.input_layernorm_weight torch.Size([2048])
layers.13.attention.q_proj_weight torch.Size([2048, 2048])
layers.13.attention.k_proj_weight torch.Size([2048, 2048])
layers.13.attention.v_proj_weight torch.Size([2048, 2048])
layers.13.attention.o_proj_weight torch.Size([2048, 2048])
layers.13.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.13.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.13.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.14.post_attention_weight torch.Size([2048])
layers.14.input_layernorm_weight torch.Size([2048])
layers.14.attention.q_proj_weight torch.Size([2048, 2048])
layers.14.attention.k_proj_weight torch.Size([2048, 2048])
layers.14.attention.v_proj_weight torch.Size([2048, 2048])
layers.14.attention.o_proj_weight torch.Size([2048, 2048])
layers.14.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.14.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.14.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.15.post_attention_weight torch.Size([2048])
layers.15.input_layernorm_weight torch.Size([2048])
layers.15.attention.q_proj_weight torch.Size([2048, 2048])
layers.15.attention.k_proj_weight torch.Size([2048, 2048])
layers.15.attention.v_proj_weight torch.Size([2048, 2048])
layers.15.attention.o_proj_weight torch.Size([2048, 2048])
layers.15.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.15.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.15.feed_forward.down_proj_weight torch.Size([2048, 8192])
```


```bash
norm_weight torch.Size([2048])
lm_head_weight torch.Size([2048, 128256])
tok_embeddings.weight torch.Size([128256, 2048])

layers.0.post_attention_weight torch.Size([2048])
layers.0.input_layernorm_weight torch.Size([2048])
layers.0.attention.q_proj_weight torch.Size([2048, 2048])
layers.0.attention.k_proj_weight torch.Size([2048, 2048])
layers.0.attention.v_proj_weight torch.Size([2048, 2048])
layers.0.attention.o_proj_weight torch.Size([2048, 2048])
layers.0.feed_forward.gate_proj_weight torch.Size([2048, 8192])
layers.0.feed_forward.up_proj_weight torch.Size([8192, 2048])
layers.0.feed_forward.down_proj_weight torch.Size([2048, 8192])

llama-3.2-1b torch weights name  dict_keys(['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight','norm.weight', 'output.weight'])
```

```bash
# 自定义模型结构的参数名称及权重
layers.14.feed_forward.gate_proj_weight torch.Size([8192, 2048])
layers.14.feed_forward.down_proj_weight torch.Size([2048, 8192])
layers.14.feed_forward.up_proj_weight torch.Size([8192, 2048])

# 训练好的模型结构的参数名称及权重
layers.14.feed_forward.w1.weight torch.Size([8192, 2048])
layers.14.feed_forward.w2.weight torch.Size([2048, 8192])
layers.14.feed_forward.w3.weight torch.Size([8192, 2048])
```